{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, I made code that could download and save tweets about Pokemon Go. I decided to do this after seeing the bad reactions on social media about the Pokemon Go Fest in Chicago on July 22nd, which was plagued by long lines, server overload, and glitchy user experience. I ran these scripts every day (except while on vacation) for about a month so that I could see if I could follow the sentiment of Pokemon Go users over time.\n",
    "\n",
    "The steps in this project were to:\n",
    "1. Search for tweets about pokemon go and save them for later analysis. I did two searches. One looked at a few different search terms individually, and the other looked for any of those search terms specifically in Chicago vs. everywhere.\n",
    "2. Training a sentiment classifier (Naive Bayes Classifier). I used the built-in NLTK twitter corpus as a gold standard at first, but then didn't feel like it was doing a great job. In particular, the NLTK corpus was established using emoticons to determine the true sentiment. The result of that is that emoticons became absurdly informative features, but I think that emoticons are less used now than emoji, so I didn't feel like the corpus was well-matched to present-day tweets.\n",
    "3. Perparing the saved tweets for analysis and applying the classifier to see trends over time.\n",
    "4. Training a second sentiment classifier. I found a massive twitter corpus online which I used to train a second sentiment classifier.\n",
    "5. Comparing the two classifiers\n",
    "6. Looking at the tweets from the best and worst days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import nltk\n",
    "import twitter\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import string\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd \"/Users/hjohnsen/Dropbox (Personal)/Data Science/Week-8-NLP-Databases/pickles\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd = !pwd\n",
    "print(pwd)\n",
    "if not pwd[0] == \"/Users/hjohnsen/Dropbox (Personal)/Data Science/Week-8-NLP-Databases\":\n",
    "    %cd \"/Users/hjohnsen/Dropbox (Personal)/Data Science/Week-8-NLP-Databases/\"    \n",
    "\n",
    "if not os.path.exists('secret_twitter_credentials.pkl'):\n",
    "    Twitter={}\n",
    "    Twitter['Consumer Key'] = ''\n",
    "    Twitter['Consumer Secret'] = ''\n",
    "    Twitter['Access Token'] = ''\n",
    "    Twitter['Access Token Secret'] = ''\n",
    "    with open('secret_twitter_credentials.pkl','wb') as f:\n",
    "        pickle.dump(Twitter, f)\n",
    "else:\n",
    "    Twitter=pickle.load(open('secret_twitter_credentials.pkl','rb'))\n",
    "    \n",
    "auth = twitter.oauth.OAuth(Twitter['Access Token'],\n",
    "                           Twitter['Access Token Secret'],\n",
    "                           Twitter['Consumer Key'],\n",
    "                           Twitter['Consumer Secret'])\n",
    "\n",
    "twitter_api = twitter.Twitter(auth=auth)\n",
    "\n",
    "%cd pickles/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I did not end up using these trends for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "worldID = 1\n",
    "usID = 23424977\n",
    "chicagoID = 2379574\n",
    "grantparkID = 12784255 #using zipcode 60601 for lookup\n",
    "\n",
    "trends = {}\n",
    "trends[\"world_trends\"] = twitter_api.trends.place(_id=worldID)\n",
    "trends[\"us_trends\"] = twitter_api.trends.place(_id=usID)\n",
    "trends[\"chicago_trends\"] = twitter_api.trends.place(_id=chicagoID)\n",
    "#not working\n",
    "#trends[\"grantpark_trends\"] = twitter_api.trends.place(_id=grantparkID)  \n",
    "\n",
    "#since trends might be time sensitive, I want to save them\n",
    "currTime = str(datetime.now())\n",
    "with open(currTime + \"trends.pkl\",'wb') as f:\n",
    "    pickle.dump(trends, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#alltrends = {}\n",
    "#for trend in trends.keys():\n",
    "#    trendlist = trends[trend][0][\"trends\"]\n",
    "#    for item in trendlist:\n",
    "#        #print(item[\"name\"] + \", \" + str(item[\"tweet_volume\"]))\n",
    "#        currentValue = alltrends.get(item[\"name\"], 0)\n",
    "#        if item[\"tweet_volume\"] is None or item[\"tweet_volume\"]==0:\n",
    "#            pass\n",
    "#        else: alltrends[item[\"name\"]] = item[\"tweet_volume\"] + currentValue\n",
    "##for item in alltrends.keys():\n",
    "##    print (item + \", \" ,alltrends[item])\n",
    "#df = pd.Series(alltrends)\n",
    "##print(df)\n",
    "#df.sort_values(ascending = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cTrends = {}\n",
    "#for item in trends[\"chicago_trends\"][0][\"trends\"]:\n",
    "#    cTrends[item[\"name\"]] = item[\"tweet_volume\"]\n",
    "#dfc = pd.Series(cTrends)\n",
    "#dfc.sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting and saving tweets about PoGo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I searched for tweets about pokemon go or the specific event. I did not realize it, but the case doesn't matter in Twitter's search, so \"#PokemonGOFest\" and \"#pokemongofest\" were identical searches.\n",
    "\n",
    "100 is the maximum number that could be found and saved with these searches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 'Pokemon Go'\n",
    "b = \"#PokemonGOFest\"\n",
    "c = \"#pokemongofest\"\n",
    "d = \"pokemongo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number = 100\n",
    "searchresults = {}\n",
    "searchresults[\"a\"] = twitter_api.search.tweets(q=a, count=number)\n",
    "searchresults[\"b\"] = twitter_api.search.tweets(q=b, count=number)\n",
    "searchresults[\"c\"] = twitter_api.search.tweets(q=c, count=number)\n",
    "searchresults[\"d\"] = twitter_api.search.tweets(q=d, count=number)\n",
    "\n",
    "currTime = str(datetime.now())\n",
    "with open(currTime + \"search.pkl\",'wb') as f:\n",
    "    pickle.dump(searchresults, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for search in searchresults.keys():\n",
    "    print(search)\n",
    "    print(len(searchresults[search][\"statuses\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting and saving tweets about PoGo in Chicago vs. elsewhere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = '-RT Pokemon Go OR #PokemonGOFest OR #pokemongofest OR pokemongo'\n",
    "# This is centered on Grant Park, where the event took place.\n",
    "loc = \"41.8722,-87.621887,100mi\"\n",
    "lang = \"en\"\n",
    "number = 100\n",
    "\n",
    "search = {}\n",
    "search[\"chicagoSearch\"] = twitter_api.search.tweets(q=q, geocode=loc, lang=lang, count = number)\n",
    "search[\"everywhereSearch\"]  = twitter_api.search.tweets(q=q, lang=lang, count = number)\n",
    "\n",
    "currTime = str(datetime.now())\n",
    "with open(currTime + \"Csearch.pkl\",'wb') as f:\n",
    "    pickle.dump(search, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in search.keys():\n",
    "    print(i)\n",
    "    print(len(search[i][\"statuses\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am not sure why the non-geocode-limited search result returns fewer queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for a given \"search\" (stored as a dictionary), \n",
    "#extract the a list of status texts for a location (a key in that dict)\n",
    "def getStatuses(d,k):\n",
    "    return [s['text'] for s in d[k]['statuses']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter, from \"Using the Twitter API for Tweet Analysis\" \n",
    "# modified since statuses here is a list of just the text\n",
    "def filterRepeats(statuses):\n",
    "    all_text = []\n",
    "    filtered_statuses = []\n",
    "    for s in statuses:\n",
    "        if not s in all_text:\n",
    "            filtered_statuses.append(s)\n",
    "            all_text.append(s)\n",
    "    return filtered_statuses     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "everywhereTexts = getStatuses(search, \"everywhereSearch\")\n",
    "#print(everywhereTexts)\n",
    "print(len(everywhereTexts))\n",
    "everywhereTextsFiltered = filterRepeats(everywhereTexts)\n",
    "print(len(everywhereTextsFiltered))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm not entirely sure why this filtere didn't seem to work (it basically  never changed the length of the tweet list), except that maybe since retweets had already been excluded, there were few identical repeats. Something I also noticed is that when there were apparent repeats, they often had URLs that were different. For example, there might be two shortened URLs that pointed to the same place, but were perhaps different so that the clicks could be tracked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "everywhereTexts[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chicagoTexts = getStatuses(search, \"chicagoSearch\")\n",
    "print(len(chicagoTexts))\n",
    "chicagoTextsFiltered = filterRepeats(chicagoTexts)\n",
    "print(len(chicagoTextsFiltered))\n",
    "#print(chicagoTextsFiltered)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I wasted a lot of time clicking on links in the tweets printed by this line :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chicagoTexts[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "everywhereTexts[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I notice that my search is also finding things that are unrelated to Pokemon Go but just have those two words in them. For example:\n",
    "\n",
    "\"@ChildrensITV It won't let me watch pokemon sun and moon on ITV Hub. It says unavailable when I click on go on the app?\"\n",
    " or \n",
    "'When you go in the tall grass without your starting pokemon. https://t.co/dg9lrnsRp0'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a twitter sentiment classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I trained a classifier in the same way that was shown in the MOOC examples. I trained it on the NLTK twitter sample corpus, which already tokenized the tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download(\"twitter_samples\")\n",
    "from nltk.corpus import twitter_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(twitter_samples.fileids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(twitter_samples.fileids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I am attempting to remove @s and URLs since they are not real, useful words.\n",
    "def build_bag_of_words_features_filtered(words):\n",
    "    bag = {}\n",
    "    useless_words = nltk.corpus.stopwords.words(\"english\") + list(string.punctuation)\n",
    "    for word in words:\n",
    "        if not word in useless_words:\n",
    "            if not \"http\" in word:\n",
    "                if not \"@\" in word:\n",
    "                    bag[word]=1\n",
    "    return bag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negstrings = twitter_samples.strings(\"negative_tweets.json\")\n",
    "#print(negstrings[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negtokens = twitter_samples.tokenized(\"negative_tweets.json\")\n",
    "postokens = twitter_samples.tokenized(\"positive_tweets.json\")\n",
    "#print(negtokens[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negbag = [build_bag_of_words_features_filtered(i) for i in negtokens]\n",
    "negfeatures = [(bag, \"neg\") for bag in negbag]\n",
    "posbag = [build_bag_of_words_features_filtered(i) for i in postokens]\n",
    "posfeatures = [(bag, \"pos\") for bag in posbag]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(len(negfeatures))\n",
    "#print(len(posfeatures))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.classify import NaiveBayesClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = 4000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_classifier = NaiveBayesClassifier.train(posfeatures[:split]+negfeatures[:split])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model seems highly accurate, but this is likely because it is somewhat overfit in that the corpus is not representative of real tweets-- they are filtered to include emoticons. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.classify.util.accuracy(sentiment_classifier, posfeatures[split:]+negfeatures[split:])*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is highly overfit for identifying emoticons, which is how neg and pos were originally defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sentiment_classifier.show_most_informative_features()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Determining positivity for the PoGo tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chicagoTexts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizeTweets(tweetList):\n",
    "    wordsList = []\n",
    "    for tweet in tweetList:\n",
    "        wordsList.append(nltk.word_tokenize(tweet))\n",
    "    return wordsList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chicagoTokens = tokenizeTweets(chicagoTexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chicagoBag = [build_bag_of_words_features_filtered(i) for i in chicagoTokens]\n",
    "#print(chicagoBag[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell prints some sample tweets along with their probability of being positive. Based on these results, I didn't feel like my classifier was doing a great job. But I also realized that many tweets didn't have a particular obvious sentiment-- many were simply giving information about game updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "classifications = []\n",
    "for tweet in chicagoBag:\n",
    "    classifications.append(sentiment_classifier.prob_classify(tweet))\n",
    "for i in range(10):\n",
    "    print(chicagoTexts[i])\n",
    "    print(classifications[i].prob(\"pos\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dir(classifications[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of actually classifying tweets as positive or negative, I use the probability of being positive as the score for the tweet. I averaged over all tweets collected in one session to get an overall approval rating for pogo or the pogo fest for that day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def approvalRating(classifList):\n",
    "    runningScore = 0\n",
    "    count = 0\n",
    "    for tweet in classifList:\n",
    "        runningScore += tweet.prob(\"pos\")\n",
    "        count += 1\n",
    "    return 100*runningScore/count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "approvalRating(classifications)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After trying out my code, I made a pipeline that could be run for each group of saved tweets to process them from raw tweets into an average approval rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if repeatFilterOn is true, then this will filter repeats out of the tweets. Otherwise, it will not.\n",
    "repeatFilterOn = False\n",
    "\n",
    "def pipeline(query):\n",
    "    scores = {}\n",
    "    for place in query.keys():\n",
    "        #print(place)\n",
    "        if repeatFilterOn:\n",
    "            statuses = filterRepeats(getStatuses(query, place))\n",
    "        else:\n",
    "            statuses = getStatuses(query, place)\n",
    "        #print(statuses[0])\n",
    "        bag = [build_bag_of_words_features_filtered(i) for i in tokenizeTweets(statuses)]\n",
    "        #print(bag[0])\n",
    "        classifications = []\n",
    "        for tweet in bag:\n",
    "            classifications.append(sentiment_classifier.prob_classify(tweet))\n",
    "        #    print(classifications[-1:])\n",
    "        nTweets = len(classifications)\n",
    "        if nTweets == 0:\n",
    "            print(\"No tweets saved; skipping\")\n",
    "        else:\n",
    "            print(\"number of tweets: \", nTweets)\n",
    "            score = approvalRating(classifications)\n",
    "            #print(place, score)\n",
    "            scores[place]=score\n",
    "    print(scores)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using saved historical tweets to find trends over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = !ls\n",
    "datetimes = []\n",
    "output = []\n",
    "for filename in files:\n",
    "    if \"search\" in filename:\n",
    "        if \"Csearch\" not in filename:\n",
    "            print(filename)\n",
    "            searchresults = pickle.load(open(filename, \"rb\"))\n",
    "            datetimes.append(datetime.strptime(filename[:-10], \"%Y-%m-%d %H:%M:%S.%f\"))\n",
    "            output.append(pipeline(searchresults))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {a:[], b:[], c:[], d:[], \"dt\":[]}\n",
    "for i in range(len(output)):\n",
    "    data[a].append(output[i][\"a\"])\n",
    "    data[b].append(output[i][\"b\"])\n",
    "    data[c].append(output[i][\"c\"])\n",
    "    data[d].append(output[i][\"d\"])\n",
    "    data[\"dt\"].append(datetimes[i])   \n",
    "datadf =  pd.DataFrame.from_dict(data)\n",
    "datadf.set_index(datadf[\"dt\"], inplace = True)\n",
    "datadf.pop(\"dt\")\n",
    "datadf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(datadf.corr())\n",
    "print()\n",
    "print(datadf.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I found it interesting that tweets related to the Go Fest event did not correlate all that well to general pokemon go tweets. Overall, the approval scores varied a lot over the month. The best day for pogo, with an approval of 85%, was 8/16. At the end of the analysis, I look into what happened that day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadf[datadf[\"pokemongo\"]>85]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I plotted the raw data as points, lines, and then also as a rolling average to smooth out the high variance. The open space is when I was on vacation and didn't collect data. There are no blue dots/lines because they are all identical to and written over by the orange (since the search is case insensitive)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "del datadf[\"#pokemongofest\"]\n",
    "datadf.plot(style=\".\", ylim=[10,90])\n",
    "plt.xlabel(\"Date of query\")\n",
    "plt.ylabel(\"Positivity\")\n",
    "datadf.plot(ylim=[10,90])\n",
    "plt.xlabel(\"Date of query\")\n",
    "plt.ylabel(\"Positivity\")\n",
    "pd.rolling_mean(datadf,3).plot(ylim=[10,90])\n",
    "plt.xlabel(\"Date of query\")\n",
    "plt.ylabel(\"Positivity\")\n",
    "#, figsize=(15,10)\n",
    "\n",
    "#plt.figure(figsize=(20,10))\n",
    "#plt.plot_date(datadf, xdate=True, ydate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm not sure why, but many of the early days found no tweets for Chicago. I can't remember now if I modified the query or if it started working better on its own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdatetimes = []\n",
    "coutput = []\n",
    "\n",
    "files = !ls\n",
    "for filename in files:\n",
    "    if \"Csearch\" in filename:\n",
    "        print(filename)\n",
    "        searchresults = pickle.load(open(filename, \"rb\"))\n",
    "        pipeline(searchresults)\n",
    "        cdatetimes.append(datetime.strptime(filename[:-11], \"%Y-%m-%d %H:%M:%S.%f\"))\n",
    "        coutput.append(pipeline(searchresults))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coutput[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdata = {\"chicagoSearch\":[], \"everywhereSearch\":[], \"dt\":[]}\n",
    "for i in range(len(coutput)):\n",
    "    if \"chicagoSearch\" not in coutput[i].keys():\n",
    "        cdata[\"chicagoSearch\"].append(0)\n",
    "    else: \n",
    "        cdata[\"chicagoSearch\"].append(coutput[i][\"chicagoSearch\"])\n",
    "    cdata[\"everywhereSearch\"].append(coutput[i][\"everywhereSearch\"])\n",
    "    cdata[\"dt\"].append(cdatetimes[i])    \n",
    "    \n",
    "cdatadf =  pd.DataFrame.from_dict(cdata)\n",
    "cdatadf.set_index(cdatadf[\"dt\"], inplace = True)\n",
    "cdatadf.pop(\"dt\")\n",
    "cdatadf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I graph using a ymin of 10, because these days with a rating of 0 are just because no tweets were collected in Chicago at those times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cdatadf.corr())\n",
    "print()\n",
    "print(cdatadf.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cdatadf.plot(style=\".\", ylim=[10,90])\n",
    "plt.xlabel(\"Date of query\")\n",
    "plt.ylabel(\"Positivity\")\n",
    "cdatadf.plot(ylim=[10,90])\n",
    "plt.xlabel(\"Date of query\")\n",
    "plt.ylabel(\"Positivity\")\n",
    "pd.rolling_mean(cdatadf,3).plot(ylim=[10,90])\n",
    "plt.xlabel(\"Date of query\")\n",
    "plt.ylabel(\"Positivity\")\n",
    "#, figsize=(15,10)\n",
    "\n",
    "#x=cdatadf.plot(style=\".\", figsize=(15,10), ylim=[40,90])\n",
    "#plt.figure(figsize=(20,10))\n",
    "#plt.plot_date(cdata[\"dt\"], cdata[\"everywhereSearch\"],label = \"everywhereSearch\", xdate=True, ydate=False)\n",
    "#plt.plot_date(cdata[\"dt\"], cdata[\"chicagoSearch\"],label = \"chicagoSearch\", xdate=True, ydate=False)\n",
    "#plt.legend()\n",
    "#plt.ylim([40,90])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Training a second twitter sentiment classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The previous classifier was probably not very accurate. It was based off a model that used emoticons to define the \"gold standard,\" so the most important features for the classifier were :) and :( emoticons by far. I expect that this makes tweets without emoticons hard to analyze. At the same time, many of the tweets that I've seen could better be thought of as informative rather than emotive, so it's hard to know what kind of sentiment it should have.\n",
    "\n",
    "Looking at some sample tweets, I don't think I would have given positive rating predictions like the classifier, for example:\n",
    "\n",
    "I liked a @YouTube video https://t.co/3mnvMLL74a This Problem with Pokémon Go NEEDS to be Solved NOW...\n",
    "0.7723498203721197\n",
    "\n",
    "I found a second corpus of tweets online to try out: http://thinknook.com/twitter-sentiment-analysis-training-corpus-dataset-2012-09-22/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd = !pwd\n",
    "print(pwd)\n",
    "if not pwd[0] == \"/Users/hjohnsen/Dropbox (Personal)/Data Science/Week-8-NLP-Databases\":\n",
    "    %cd \"/Users/hjohnsen/Dropbox (Personal)/Data Science/Week-8-NLP-Databases/\"   \n",
    "trainingData=pd.read_csv(\"SentimentAnalysisDataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del trainingData[\"ItemID\"]\n",
    "del trainingData[\"SentimentSource\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainingData.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I found an NLTK tokenizer that is designed for tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tknzr = nltk.tokenize.casual.TweetTokenizer(preserve_case=False)\n",
    "#tknzr.tokenize(trainingData[\"SentimentText\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingData[\"tokenizedbag\"]=trainingData[\"SentimentText\"].map(tknzr.tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negData = trainingData[trainingData[\"Sentiment\"]==0][\"tokenizedbag\"]\n",
    "posData = trainingData[trainingData[\"Sentiment\"]==1][\"tokenizedbag\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negbag = [(build_bag_of_words_features_filtered(i), \"neg\") for i in negData]\n",
    "posbag = [(build_bag_of_words_features_filtered(i), \"pos\") for i in posData]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(len(negbag))\n",
    "#print(len(posbag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.classify import NaiveBayesClassifier\n",
    "nsplit = int(.8*len(negbag))\n",
    "psplit = int(.8*len(posbag))\n",
    "\n",
    "sentiment_classifier2 = NaiveBayesClassifier.train(posbag[:psplit]+negbag[:nsplit])\n",
    "print(\"Score on training set:\")\n",
    "print(nltk.classify.util.accuracy(sentiment_classifier2, posbag[:psplit]+negbag[:nsplit])*100)\n",
    "print(\"Score on test set:\")\n",
    "print(nltk.classify.util.accuracy(sentiment_classifier2, posbag[psplit:]+negbag[nsplit:])*100)\n",
    "\n",
    "sentiment_classifier2.show_most_informative_features()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While this sentiment classifier appears to have less accuracy, it's probably a more believable value than the other one's 99% accuracy (especially given that humans are only about 80% accurate). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the new classifier to do the same analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd = !pwd\n",
    "print(pwd)\n",
    "if not pwd[0] == \"/Users/hjohnsen/Dropbox (Personal)/Data Science/Week-8-NLP-Databases/pickles\":\n",
    "    %cd \"/Users/hjohnsen/Dropbox (Personal)/Data Science/Week-8-NLP-Databases/pickles\"  \n",
    "# !ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repeatFilterOn = True\n",
    "# I will tokenize using the same method as my training set this time.\n",
    "def tokenizeTweets2(tweetList):\n",
    "    wordsList = []\n",
    "    for tweet in tweetList:\n",
    "        wordsList.append(tknzr.tokenize(tweet))\n",
    "    return wordsList\n",
    "\n",
    "#chicagoTokens = tokenizeTweets(chicagoTexts)\n",
    "\n",
    "#chicagoBag = [build_bag_of_words_features_filtered(i) for i in chicagoTokens]\n",
    "\n",
    "#classifications = []\n",
    "#for tweet in chicagoBag:\n",
    "#    classifications.append(sentiment_classifier.prob_classify(tweet))\n",
    "#for i in range(10):\n",
    "#    print(chicagoTexts[i])\n",
    "#    print(classifications[i].prob(\"pos\"))\n",
    "\n",
    "def approvalRating(classifList):\n",
    "    runningScore = 0\n",
    "    count = 0\n",
    "    for tweet in classifList:\n",
    "        runningScore += tweet.prob(\"pos\")\n",
    "        count += 1\n",
    "    return 100*runningScore/count\n",
    "\n",
    "#approvalRating(classifications)\n",
    "\n",
    "def pipeline2(query):\n",
    "    scores = {}\n",
    "    for place in query.keys():\n",
    "        #print(place)\n",
    "        if repeatFilterOn:\n",
    "            statuses = filterRepeats(getStatuses(query, place))\n",
    "        else:\n",
    "            statuses = getStatuses(query, place)\n",
    "        #print(statuses[0])\n",
    "        bag = [build_bag_of_words_features_filtered(i) for i in tokenizeTweets2(statuses)]\n",
    "        #print(bag[0])\n",
    "        classifications = []\n",
    "        for tweet in bag:\n",
    "            classifications.append(sentiment_classifier2.prob_classify(tweet))\n",
    "        #    print(classifications[-1:])\n",
    "        nTweets = len(classifications)\n",
    "        if nTweets == 0:\n",
    "            print(\"No tweets saved; skipping\")\n",
    "        else:\n",
    "            score = approvalRating(classifications)\n",
    "            #print(\"number of tweets: \", nTweets)\n",
    "            #print(place, score)\n",
    "            scores[place]=score\n",
    "    print(scores)\n",
    "    return scores\n",
    "\n",
    "# Using saved historical tweets to find trends over time\n",
    "\n",
    "files = !ls\n",
    "datetimes = []\n",
    "output = []\n",
    "for filename in files:\n",
    "    if \"search\" in filename:\n",
    "        if \"Csearch\" not in filename:\n",
    " #           print(filename)\n",
    "            searchresults = pickle.load(open(filename, \"rb\"))\n",
    "            datetimes.append(datetime.strptime(filename[:-10], \"%Y-%m-%d %H:%M:%S.%f\"))\n",
    "            output.append(pipeline2(searchresults))\n",
    "\n",
    "\n",
    "data2 = {a:[], b:[], c:[], d:[], \"dt\":[]}\n",
    "for i in range(len(output)):\n",
    "    data2[a].append(output[i][\"a\"])\n",
    "    data2[b].append(output[i][\"b\"])\n",
    "    data2[c].append(output[i][\"c\"])\n",
    "    data2[d].append(output[i][\"d\"])\n",
    "    data2[\"dt\"].append(datetimes[i])   \n",
    "datadf2 =  pd.DataFrame.from_dict(data2)\n",
    "datadf2.set_index(datadf2[\"dt\"], inplace = True)\n",
    "datadf2.pop(\"dt\")\n",
    "datadf2.head()\n",
    "\n",
    "print(datadf2.corr())\n",
    "print()\n",
    "print(datadf2.describe())\n",
    "\n",
    "#datadf[datadf[\"pokemongo\"]>85]\n",
    "\n",
    "\n",
    "cdatetimes = []\n",
    "coutput = []\n",
    "\n",
    "files = !ls\n",
    "for filename in files:\n",
    "    if \"Csearch\" in filename:\n",
    "        print(filename)\n",
    "        searchresults = pickle.load(open(filename, \"rb\"))\n",
    "        pipeline2(searchresults)\n",
    "        cdatetimes.append(datetime.strptime(filename[:-11], \"%Y-%m-%d %H:%M:%S.%f\"))\n",
    "        coutput.append(pipeline2(searchresults))\n",
    "\n",
    "coutput[:5]\n",
    "\n",
    "cdata2 = {\"chicagoSearch\":[], \"everywhereSearch\":[], \"dt\":[]}\n",
    "for i in range(len(coutput)):\n",
    "    if \"chicagoSearch\" not in coutput[i].keys():\n",
    "        cdata2[\"chicagoSearch\"].append(0)\n",
    "    else: \n",
    "        cdata2[\"chicagoSearch\"].append(coutput[i][\"chicagoSearch\"])\n",
    "    cdata2[\"everywhereSearch\"].append(coutput[i][\"everywhereSearch\"])\n",
    "    cdata2[\"dt\"].append(cdatetimes[i])    \n",
    "    \n",
    "cdatadf2 =  pd.DataFrame.from_dict(cdata2)\n",
    "cdatadf2.set_index(cdatadf2[\"dt\"], inplace = True)\n",
    "cdatadf2.pop(\"dt\")\n",
    "cdatadf2.head()\n",
    "\n",
    "print(cdatadf2.corr())\n",
    "print()\n",
    "print(cdatadf2.describe())\n",
    "\n",
    "#, figsize=(15,10)\n",
    "\n",
    "#x=cdatadf.plot(style=\".\", figsize=(15,10), ylim=[40,90])\n",
    "#plt.figure(figsize=(20,10))\n",
    "#plt.plot_date(cdata[\"dt\"], cdata[\"everywhereSearch\"],label = \"everywhereSearch\", xdate=True, ydate=False)\n",
    "#plt.plot_date(cdata[\"dt\"], cdata[\"chicagoSearch\"],label = \"chicagoSearch\", xdate=True, ydate=False)\n",
    "#plt.legend()\n",
    "#plt.ylim([40,90])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del datadf2[\"#pokemongofest\"]\n",
    "datadf2.plot(style=\".\", ylim=[10,90])\n",
    "plt.xlabel(\"Date of query\")\n",
    "plt.ylabel(\"Positivity\")\n",
    "datadf2.plot(ylim=[10,90])\n",
    "plt.xlabel(\"Date of query\")\n",
    "plt.ylabel(\"Positivity\")\n",
    "pd.rolling_mean(datadf2,3).plot(ylim=[10,90])\n",
    "plt.xlabel(\"Date of query\")\n",
    "plt.ylabel(\"Positivity\")\n",
    "#, figsize=(15,10)\n",
    "\n",
    "#plt.figure(figsize=(20,10))\n",
    "#plt.plot_date(datadf, xdate=True, ydate=False)\n",
    "\n",
    "#plt.figure(figsize=(20,10))\n",
    "#plt.plot_date(data[\"dt\"], data[a],label = a, xdate=True, ydate=False)\n",
    "#plt.plot_date(data[\"dt\"], data[b],label = b, xdate=True, ydate=False)\n",
    "#plt.plot_date(data[\"dt\"], data[c],label = c, xdate=True, ydate=False)\n",
    "#plt.plot_date(data[\"dt\"], data[d],label = d, xdate=True, ydate=False)\n",
    "#plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdatadf2.plot(style=\".\", ylim=[10,90])\n",
    "plt.xlabel(\"Date of query\")\n",
    "plt.ylabel(\"Positivity\")\n",
    "cdatadf2.plot(ylim=[10,90])\n",
    "plt.xlabel(\"Date of query\")\n",
    "plt.ylabel(\"Positivity\")\n",
    "pd.rolling_mean(cdatadf2,3).plot(ylim=[10,90])\n",
    "plt.xlabel(\"Date of query\")\n",
    "plt.ylabel(\"Positivity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing the two classifiers\n",
    "To get a sense of which classifier performed better, I asked them to each classify some tweets. I did this on the chicagoBag because it was already easily available in the memory, however it's probably important to note that it is not tokenized in the same way as either of these models' training data were, which might skew things. I took this information and tried to classify some of the tweets as positive or negative myself to calculate the RMSE and log-likelihood for each classifier to see which is better. They both had several hits and misses with pretty similar RMSEs (0.480237111\tvs 0.488219941 for the first and second classifier, respectively) and log-likelihoods (-7.958536425 vs\t-7.708474697). \n",
    "\n",
    "Although based on these niether classifier seems hugely better than the other, if I had to choose one for further use I would probably use the second given that it was based on a more representative and much larger dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i,tweet in enumerate(chicagoBag[:10]):\n",
    "    print(chicagoTexts[i])\n",
    "    print(sentiment_classifier.prob_classify(tweet).prob(\"pos\"))\n",
    "    print(sentiment_classifier2.prob_classify(tweet).prob(\"pos\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Exploring specific days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd = !pwd\n",
    "if not pwd[0] == \"/Users/hjohnsen/Dropbox (Personal)/Data Science/Week-8-NLP-Databases/pickles\":\n",
    "    %cd \"/Users/hjohnsen/Dropbox (Personal)/Data Science/Week-8-NLP-Databases/pickles\"  \n",
    "# !ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Earlier, I found that approval of pogo peaked on August 16th. I couldn't identify clearly what announcements had been made that day by searching the web, so I wanted to look at the tweets themselves:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweetTester(tweet):\n",
    "    tokenized = tokenizeTweets([tweet])\n",
    "    bag = build_bag_of_words_features_filtered(tokenized[0])\n",
    "    print(sentiment_classifier2.prob_classify(bag).prob(\"pos\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "peakday=\"2017-08-16 16:45:17.978716search.pkl\"\n",
    "\n",
    "peakdaytweets = pickle.load(open(peakday, \"rb\"))\n",
    "for key in peakdaytweets.keys():\n",
    "    rtcount = 0\n",
    "    othercount = 0\n",
    "    peakstatuses= getStatuses(peakdaytweets, key)\n",
    "    print(key+ \"=\"*100)\n",
    "    for i in peakstatuses:\n",
    "        print(i)\n",
    "        if \"RT @Pokemon: Spotted: Shiny Pikachu,\" in i:\n",
    "           rtcount +=1\n",
    "        else:\n",
    "            othercount +=1\n",
    "        tweetTester(i)\n",
    "    print(rtcount)\n",
    "    print(othercount)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pogo's \"best\" day was a day when the shiny versions of the pikachu family came out. The tweet \"RT @Pokemon: Spotted: Shiny Pikachu, Pichu, and Raichu in #PokemonGO! Be on the lookout for these Shiny versions as you explore:\" was highly positive and retweeted a lot! (This search didn't filter out retweets, unlike the Chicago vs Everywhere query.) With numerous retweets of this very positive post, it was a good day according to the sentiment classifier.\n",
    "\n",
    "Next I looked at some of the tweets on the first day, which didn't have quite as bad of a score as I expected given all the tweets and posts I was reading online that day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "firstday=\"2017-07-22 19:04:57.902835search.pkl\"\n",
    "firstdaytweets = pickle.load(open(firstday, \"rb\"))\n",
    "for key in firstdaytweets.keys():\n",
    "    firststatuses= getStatuses(firstdaytweets, key)\n",
    "    print(key+ \"=\"*100)\n",
    "    for i in firststatuses:\n",
    "        print(i)\n",
    "        tweetTester(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I realized that I didn't filter out retweets or foreign languages in this search query, but my sentiment analyzer has no idea what to do with that. I was curious why my analyzer was even giving it a positive score instead of giving a 0.5 or something, and I found that just \"RT\" on its own seems to be a positive feature.\n",
    "\n",
    "Additionally, by this time in the day, legendary pokemon had been announced, which buffered their approval somewhat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet = \"RT @famitsu: 『Pokemon GO』に伝説のポケモン“ルギア”、“フリーザー”が登場！ 　皆の力を結集し、伝説のポケモンをゲットしよう！  https://t.co/dIa4e5AE5g https://t.co/bGqgqE9xr3\"\n",
    "tweetTester(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet = \"RT\"\n",
    "tweetTester(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet = \"米国のポケモンGOイベントで大規模サーバ障害が発生、運営元ナイアンティックはチケット全額返金と100ドル分の詫びコイン、伝説のポケモン『ルギア』をイベント出席者全員に配布する対応を発表しました\"\n",
    "tweetTester(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "firstday=\"2017-07-22 15:32:06.903443search.pkl\"\n",
    "firstdaytweets = pickle.load(open(firstday, \"rb\"))\n",
    "for key in firstdaytweets.keys():\n",
    "    firststatuses= getStatuses(firstdaytweets, key)\n",
    "    print(key+ \"=\"*100)\n",
    "    for i in firststatuses:\n",
    "        print(i)\n",
    "        tweetTester(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "firstday=\"2017-07-22 17:40:45.802043search.pkl\"\n",
    "firstdaytweets = pickle.load(open(firstday, \"rb\"))\n",
    "for key in firstdaytweets.keys():\n",
    "    firststatuses= getStatuses(firstdaytweets, key)\n",
    "    print(key+ \"=\"*100)\n",
    "    for i in firststatuses:\n",
    "        print(i)\n",
    "        tweetTester(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Earlier in the day you can see some of hte negative tweets. I think the worst was probably \"Pretty sad how bad the Pokemon Go Chicago Event turned out. Cellular lines jammed up everywhere &amp; people boo-ing the CEO of Niantic\" with a 3e-05 probability of being positive!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadf[:10].plot(style=\".\", ylim=[10,90])\n",
    "datadf[:10].plot(ylim=[10,90])\n",
    "pd.rolling_mean(datadf[:10],3).plot(ylim=[10,90])\n",
    "cdatadf[:10].plot(style=\".\", ylim=[10,90])\n",
    "cdatadf[:10].plot(ylim=[10,90])\n",
    "pd.rolling_mean(cdatadf[:10],3).plot(ylim=[10,90])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This line is commented out because there is too much text in this notebook otherwise\n",
    "\n",
    "#sentiment_classifier2.show_most_informative_features(10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the above, retweets are positve:\n",
    "\n",
    "                    rt = 1                 pos : neg    =      2.7 : 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    tokenized = tokenizeTweets([\"RT\"])\n",
    "    bag = build_bag_of_words_features_filtered(tokenized[0])\n",
    "    print(sentiment_classifier2.prob_classify(bag).prob(\"pos\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    tokenized = tokenizeTweets([\"RT\"])\n",
    "    bag = build_bag_of_words_features_filtered(tokenized[0])\n",
    "    print(sentiment_classifier.prob_classify(bag).prob(\"pos\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(datadf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    tokenized = tokenizeTweets([\"RT\"])\n",
    "    bag = build_bag_of_words_features_filtered(tokenized[0])\n",
    "    print(sentiment_classifier.classify(bag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = []\n",
    "y = []\n",
    "for i,tweet in enumerate(chicagoBag[:50]):\n",
    "    print(chicagoTexts[i])\n",
    "    print(sentiment_classifier.prob_classify(tweet).prob(\"pos\"))\n",
    "    print(sentiment_classifier2.prob_classify(tweet).prob(\"pos\"))\n",
    "    x.append(sentiment_classifier.prob_classify(tweet).prob(\"pos\"))\n",
    "    y.append(sentiment_classifier2.prob_classify(tweet).prob(\"pos\"))\n",
    "plt.scatter(x,y)\n",
    "plt.xlabel(\"Classifier 1 positivity score\")\n",
    "plt.ylabel(\"Classifier 2 positivity score\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
